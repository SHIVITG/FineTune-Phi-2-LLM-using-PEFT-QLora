{"cells":[{"cell_type":"markdown","metadata":{},"source":["*Presented By:*\n","\n","**Shivani Tyagi**\n","---\n","\n","# Fine-tuning Large Language Models (LLMs) Using QLoRA\n","\n","## Introduction\n","Fine-tuning Large Language Models (LLMs) is a crucial step in adapting these powerful models to specific tasks or domains. In this seminar code tutorial, we will explore how to perform fine-tuning using QLoRA (Quantized LoRA), a memory-efficient iteration of LoRA (Low-Rank Adaptation), for parameter-efficient fine-tuning.\n","\n","## What is QLoRA?\n","QLoRA is an advancement in the fine-tuning method introduced by LoRA. It further optimizes memory usage by quantizing the weights of the LoRA adapters to lower precision, typically 4-bit instead of 8-bit. Despite the reduction in precision, QLoRA maintains a comparable level of effectiveness to LoRA.\n","\n","## Benefits of QLoRA\n","\n","1. **Memory Efficiency:** QLoRA significantly reduces the memory footprint of the fine-tuned model by quantizing weights to lower precision.\n","3. **Storage Requirements:** With reduced precision, the storage requirements for the fine-tuned model are further decreased.\n","4. **Comparable Performance:** Despite the reduction in precision, QLoRA maintains a similar level of effectiveness as LoRA, making it an attractive option for memory-constrained environments.\n","\n","## Steps for Fine-tuning LLMs Using QLoRA\n","\n","1. **Select a Pre-trained Model:** Choose a suitable pre-trained LLM that aligns with the desired architecture and functionalities for your task.\n","    \n","2. **Gather Relevant Dataset:** Collect a dataset that is labeled or structured in a way that the model can learn from it and is relevant to your task.\n","    \n","3. **Preprocess Dataset:** Preprocess the dataset by cleaning it, splitting it into training, validation, and test sets, and ensuring compatibility with the chosen pre-trained LLM.\n","    \n","4. **Fine-tuning with QLoRA:** Fine-tune the selected pre-trained LLM using QLoRA. During this process, the weights of the LoRA adapters are quantized to lower precision, reducing memory requirements while maintaining performance.\n","    \n","5. **Task-specific Adaptation:** Adjust the model's parameters based on the new dataset, allowing it to better understand and generate content relevant to the specific task.\n","    \n","6. **Evaluation:** Evaluate the fine-tuned model on relevant metrics to assess its performance and effectiveness for the task at hand.\n","\n","## Conclusion\n","QLoRA offers a memory-efficient approach to fine-tuning Large Language Models, making them more accessible and practical for deployment in memory-constrained environments. By quantizing the weights of LoRA adapters to lower precision, QLoRA strikes a balance between memory efficiency and model performance, enabling efficient adaptation of LLMs to various tasks and domains.\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["**In this notebook and tutorial, we will fine-tune [Microsoft's Phi-2](https://huggingface.co/microsoft/phi-2) relatively small 2.7B model - which has \"showcased a nearly state-of-the-art performance among models with less than 13 billion parameters\"**\n","\n","**Here we will use [QLoRA (Efficient Finetuning of Quantized LLMs)](https://arxiv.org/abs/2305.14314), a highly efficient fine-tuning technique that involves quantizing a pretrained LLM to just 4 bits and adding small “Low-Rank Adapters”. This unique approach allows for fine-tuning LLMs using just a single GPU! This technique is supported by the [PEFT library](https://huggingface.co/docs/peft/index).**\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Table of Contents"]},{"cell_type":"markdown","metadata":{},"source":["- [1- Install all the required libraries](#1)\n","- [ 2 - Loading dataset](#2)\n","- [ 3 - Create bitsandbytes configuration](#3)\n","- [ 4 - Load Base Model](#4)\n","- [ 5 - Tokenization](#5)\n","- [ 6 - Test the Model with Zero Shot Inferencing](#6)\n","- [ 7 - Pre-processing dataset](#7)\n","- [ 8 - Setup the PEFT/LoRA model for Fine-Tuning](#8)\n","- [ 9 - Train PEFT Adapter](#9)\n","- [ 10 - Evaluate the Model Qualitatively (Human Evaluation)](#10)\n","- [ 11 - Evaluate the Model Quantitatively (with ROUGE Metric)](#11)"]},{"cell_type":"markdown","metadata":{},"source":["#### Before we begin: A note on OOM errors\n","\n","If you get an error like this: `OutOfMemoryError: CUDA out of memory`, tweak your parameters to make the model less computationally intensive\n","To re-try after you tweak your parameters, open a Terminal ('Launcher' or '+' in the nav bar above -> Other -> Terminal) and run the command `nvidia-smi`. Then find the process ID `PID` under `Processes` and run the command `kill [PID]`. You will need to re-start your notebook from the beginning."]},{"cell_type":"markdown","metadata":{},"source":["<a name='1'></a>\n","#### 1. Installing and Importing all the required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl"]},{"cell_type":"markdown","metadata":{},"source":["<a name='1.1'></a>\n","#### 1.1 Weights & Biases\n","\n","This code cell disables Weights and Biases (W&B) integration by setting the environment variable `WANDB_DISABLED` to `\"true\"` using the `os.environ` module. \n","\n","### Purpose\n","Weights and Biases is a popular tool for experiment tracking and visualization in machine learning projects. Disabling W&B integration may be necessary in certain cases, such as when running experiments in environments where W&B is not available or when troubleshooting issues related to W&B integration.\n","\n","### How to Use\n","To use this code cell, simply run it in your Python environment before running any code that involves Weights and Biases. This will ensure that W&B integration is disabled for subsequent code execution.\n","\n","### Note\n","Make sure to review your project's requirements and dependencies before disabling Weights and Biases integration. Disabling W&B may affect experiment tracking and visualization capabilities if your project relies on W&B for these purposes.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T17:33:15.535412Z","iopub.status.busy":"2024-02-27T17:33:15.534568Z","iopub.status.idle":"2024-02-27T17:33:15.539380Z","shell.execute_reply":"2024-02-27T17:33:15.538492Z","shell.execute_reply.started":"2024-02-27T17:33:15.535377Z"},"trusted":true},"outputs":[],"source":["import os\n","# disable Weights and Biases\n","os.environ['WANDB_DISABLED']=\"true\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install pyarrow==11.0.0"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:33:15.109674Z","iopub.status.busy":"2024-02-27T08:33:15.108873Z","iopub.status.idle":"2024-02-27T08:33:43.304847Z","shell.execute_reply":"2024-02-27T08:33:43.303886Z","shell.execute_reply.started":"2024-02-27T08:33:15.109632Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-27 08:33:25.802269: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-27 08:33:25.802418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-27 08:33:25.982626: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"]},{"name":"stdout","output_type":"stream","text":["Token:  ·····································\n","Add token as git credential? (Y/n)  Y\n"]},{"name":"stdout","output_type":"stream","text":["Token is valid (permission: read).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    AutoTokenizer,\n","    TrainingArguments,\n","    Trainer,\n","    GenerationConfig\n",")\n","from tqdm import tqdm\n","from trl import SFTTrainer\n","import torch\n","import time\n","import pandas as pd\n","import numpy as np\n","from huggingface_hub import interpreter_login\n","\n","interpreter_login()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:33:48.990998Z","iopub.status.busy":"2024-02-27T08:33:48.990622Z","iopub.status.idle":"2024-02-27T08:33:48.996891Z","shell.execute_reply":"2024-02-27T08:33:48.995656Z","shell.execute_reply.started":"2024-02-27T08:33:48.990966Z"},"trusted":true},"outputs":[],"source":["from pynvml import *\n","\n","def print_gpu_utilization():\n","    nvmlInit()\n","    handle = nvmlDeviceGetHandleByIndex(0)\n","    info = nvmlDeviceGetMemoryInfo(handle)\n","    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"]},{"cell_type":"markdown","metadata":{},"source":["<a name='2'></a>\n","#### 2. Loading dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:33:49.794705Z","iopub.status.busy":"2024-02-27T08:33:49.794305Z","iopub.status.idle":"2024-02-27T08:33:52.644528Z","shell.execute_reply":"2024-02-27T08:33:52.643560Z","shell.execute_reply.started":"2024-02-27T08:33:49.794674Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cda32fd7a73949a1a3a384ebf909a976","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3dc7d808b46e43d8b1ac3a2c18f7ae7e","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/1.81M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e39899cc6b7478fafe0711163b04a41","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/441k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b244708eb66641e9a9077a442cb21e87","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/447k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c4aeac3ef574566b87a454b1a9842d8","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cf6b34f3403401b95cc2b05b4c91926","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35af8af0133a4448a8e78a8af7b0e66c","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 1999\n","    })\n","    validation: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 499\n","    })\n","    test: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 499\n","    })\n","})"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# https://huggingface.co/datasets/neil-code/dialogsum-test\n","huggingface_dataset_name = \"neil-code/dialogsum-test\"\n","dataset = load_dataset(huggingface_dataset_name)\n","dataset"]},{"cell_type":"markdown","metadata":{},"source":["This is what the data looks like:"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:33:52.646266Z","iopub.status.busy":"2024-02-27T08:33:52.645985Z","iopub.status.idle":"2024-02-27T08:33:52.652931Z","shell.execute_reply":"2024-02-27T08:33:52.651997Z","shell.execute_reply.started":"2024-02-27T08:33:52.646241Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'id': 'train_0',\n"," 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n"," 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n"," 'topic': 'get a check-up'}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["dataset['train'][0]"]},{"cell_type":"markdown","metadata":{},"source":["<a name='3'></a>\n","#### 3. Create bitsandbytes configuration\n","\n","**To load the model, we need a configuration class that specifies how we want the quantization to be performed. We’ll achieve this with BitesAndBytesConfig from the Transformers library. This will allow us to load our LLM in 4 bits. This way, we can divide the used memory by 4 and import the model on smaller devices.**"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:33:52.654390Z","iopub.status.busy":"2024-02-27T08:33:52.654107Z","iopub.status.idle":"2024-02-27T08:33:52.668445Z","shell.execute_reply":"2024-02-27T08:33:52.667625Z","shell.execute_reply.started":"2024-02-27T08:33:52.654366Z"},"trusted":true},"outputs":[],"source":["compute_dtype = getattr(torch, \"float16\")\n","bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type='nf4',\n","        bnb_4bit_compute_dtype=compute_dtype,\n","        bnb_4bit_use_double_quant=False,\n","    )\n","device_map = {\"\": 0}"]},{"cell_type":"markdown","metadata":{},"source":["<a name='4'></a>\n","#### 4. Load Base Model\n","Let's now load Phi-2 using 4-bit quantization!"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["model_name='microsoft/phi-2'\n","original_model = AutoModelForCausalLM.from_pretrained(model_name, \n","                                                      device_map=device_map,\n","                                                      quantization_config=bnb_config,\n","                                                      trust_remote_code=True,\n","                                                      use_auth_token=True)"]},{"cell_type":"markdown","metadata":{},"source":["<a name='5'></a>\n","#### 5. Tokenization\n","Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa)."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:34:26.549341Z","iopub.status.busy":"2024-02-27T08:34:26.548961Z","iopub.status.idle":"2024-02-27T08:34:28.040204Z","shell.execute_reply":"2024-02-27T08:34:28.038755Z","shell.execute_reply.started":"2024-02-27T08:34:26.549304Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18e58a74d5e2464783e2232e9004309c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ea2186df536458a9ac6e94afe08cb6b","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b038c41bb43b4cb499681c83d452b504","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5148a7d10a354e47a94f7ecf19bce656","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f18b58f2416437287dd06c47c38dc96","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df1b7636ed7a44acb8205356f1fd5af0","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n","eval_tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n","eval_tokenizer.pad_token = eval_tokenizer.eos_token"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:34:28.042366Z","iopub.status.busy":"2024-02-27T08:34:28.041549Z","iopub.status.idle":"2024-02-27T08:34:35.858932Z","shell.execute_reply":"2024-02-27T08:34:35.857874Z","shell.execute_reply.started":"2024-02-27T08:34:28.042326Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU memory occupied: 2318 MB.\n"]}],"source":["print_gpu_utilization()"]},{"cell_type":"markdown","metadata":{},"source":["\n","This function `gen` is for generating text using a pre-trained language model. The function takes three main parameters:\n","\n","1. `model`: The pre-trained language model to use for text generation.\n","2. `p`: The prompt or input text for text generation.\n","3. `maxlen`: The maximum length of the generated text (default is 100 tokens).\n","4. `sample`: A boolean flag indicating whether to use sampling during text generation (default is True).\n","\n","The function first tokenizes the input prompt using an evaluation tokenizer (`eval_tokenizer`) and converts the tokens to PyTorch tensors. It then generates text using the provided model, with options for sampling, beam search, and temperature control. The generated text is returned as a tensor on the CPU, and special tokens are skipped during decoding.\n","\n","### Parameters\n","- `model`: A pre-trained language model (e.g., GPT, BERT).\n","- `p`: Prompt or input text for text generation.\n","- `maxlen`: Maximum length of the generated text.\n","- `sample`: Boolean flag for sampling during text generation.\n","\n","### Usage\n","```python\n","# Example usage\n","generated_text = gen(model, \"Input prompt for text generation\", maxlen=150, sample=True)\n","print(generated_text)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:34:35.860959Z","iopub.status.busy":"2024-02-27T08:34:35.860329Z","iopub.status.idle":"2024-02-27T08:34:36.105529Z","shell.execute_reply":"2024-02-27T08:34:36.104243Z","shell.execute_reply.started":"2024-02-27T08:34:35.860918Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["def gen(model,p, maxlen=100, sample=True):\n","    toks = eval_tokenizer(p, return_tensors=\"pt\")\n","    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n","    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)"]},{"cell_type":"markdown","metadata":{},"source":["<a name='6'></a>\n","#### 6. Test the Model with Zero Shot Inferencing\n","\n","\n","1. Imports necessary modules and sets a random seed for reproducibility.\n","2. Retrieves a dialogue and its corresponding summary from the test dataset at a specified index.\n","3. Formats the dialogue prompt and uses it to generate a summary using a pre-trained language model.\n","4. Prints the input prompt, baseline human summary, and model-generated summary for comparison."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:34:36.110716Z","iopub.status.busy":"2024-02-27T08:34:36.109977Z","iopub.status.idle":"2024-02-27T08:34:43.023138Z","shell.execute_reply":"2024-02-27T08:34:43.022091Z","shell.execute_reply.started":"2024-02-27T08:34:36.110684Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","Instruct: Summarize the following conversation.\n","#Person1#: Happy Birthday, this is for you, Brian.\n","#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n","#Person1#: Brian, may I have a pleasure to have a dance with you?\n","#Person2#: Ok.\n","#Person1#: This is really wonderful party.\n","#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n","#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n","#Person2#: You look great, you are absolutely glowing.\n","#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n","Output:\n","\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n","\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ZERO SHOT:\n","Person1 and Person2 are at a party, and Person1 asks if they can have a dance. Person2 agrees and compliments Person1 on their appearance. Person1 thanks them and expresses their happiness with the party. Person2 agrees that it's a great party and suggests having a drink to celebrate.\n","\n","CPU times: user 5.86 s, sys: 274 ms, total: 6.14 s\n","Wall time: 6.9 s\n"]}],"source":["%%time\n","from transformers import set_seed\n","seed = 42\n","set_seed(seed)\n","\n","index = 10\n","\n","prompt = dataset['test'][index]['dialogue']\n","summary = dataset['test'][index]['summary']\n","\n","formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n","res = gen(original_model,formatted_prompt,100,)\n","#print(res[0])\n","output = res[0].split('Output:\\n')[1]\n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{formatted_prompt}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n","print(dash_line)\n","print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"]},{"cell_type":"markdown","metadata":{},"source":["---\n","Observations:\n","\n","The generated summary captures the essence of the conversation between Person1 and Person2 at Brian's birthday party. It includes key elements such as the invitation to dance, compliments on appearance, expressions of happiness about the party, and the suggestion to have a drink together to celebrate Brian's birthday. Overall, the model-generated summary effectively conveys the main points of the conversation, demonstrating an understanding of the dialogue context.\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["<a name='7'></a>\n","#### 7. Pre-processing dataset\n","\n","\n","The function, `create_prompt_formats`, formats various fields of a sample dictionary containing instructions, conversation dialogue, and a summary. It concatenates the formatted fields using two newline characters to create a prompt for a language model to generate a response.\n","\n","### Parameters\n","- `sample`: A dictionary containing fields such as 'instruction', 'dialogue', and 'summary'.\n","\n","### Function Steps\n","1. Define introductory blurb, instruction key, response key, and end key strings.\n","2. Format the introductory blurb, instruction, input context (dialogue), response, and end parts.\n","3. Concatenate the formatted parts using two newline characters.\n","4. Update the 'text' field of the sample dictionary with the formatted prompt.\n","5. Return the updated sample dictionary."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:34:43.024622Z","iopub.status.busy":"2024-02-27T08:34:43.024318Z","iopub.status.idle":"2024-02-27T08:34:43.031747Z","shell.execute_reply":"2024-02-27T08:34:43.030817Z","shell.execute_reply.started":"2024-02-27T08:34:43.024596Z"},"trusted":true},"outputs":[],"source":["def create_prompt_formats(sample):\n","    \"\"\"\n","    Format various fields of the sample ('instruction','output')\n","    Then concatenate them using two newline characters \n","    :param sample: Sample dictionnary\n","    \"\"\"\n","    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n","    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n","    RESPONSE_KEY = \"### Output:\"\n","    END_KEY = \"### End\"\n","    \n","    blurb = f\"\\n{INTRO_BLURB}\"\n","    instruction = f\"{INSTRUCTION_KEY}\"\n","    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n","    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n","    end = f\"{END_KEY}\"\n","    \n","    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n","\n","    formatted_prompt = \"\\n\\n\".join(parts)\n","    sample[\"text\"] = formatted_prompt\n","\n","    return sample"]},{"cell_type":"markdown","metadata":{},"source":["### `get_max_length(model)`\n","- **Description**: This function retrieves the maximum length setting from the configuration of a given model. It checks for various possible length settings and defaults to a maximum length of 1024 if none are found.\n","- **Parameters**:\n","  - `model`: The model for which the maximum length is being retrieved.\n","- **Returns**: \n","  - `max_length`: The maximum length setting for tokenization.\n","\n","### `preprocess_batch(batch, tokenizer, max_length)`\n","- **Description**: This function preprocesses a batch of text data for tokenization using a given tokenizer. It tokenizes the text in the batch and applies truncation if necessary to ensure that the length does not exceed the specified maximum length.\n","- **Parameters**:\n","  - `batch`: A batch of text data to be preprocessed.\n","  - `tokenizer`: The tokenizer used for tokenization.\n","  - `max_length`: The maximum length allowed for tokenization.\n","- **Returns**: \n","  - Tokenized and preprocessed batch."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:34:43.033369Z","iopub.status.busy":"2024-02-27T08:34:43.032963Z","iopub.status.idle":"2024-02-27T08:34:45.768994Z","shell.execute_reply":"2024-02-27T08:34:45.767787Z","shell.execute_reply.started":"2024-02-27T08:34:43.033330Z"},"trusted":true},"outputs":[],"source":["# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n","def get_max_length(model):\n","    conf = model.config\n","    max_length = None\n","    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n","        max_length = getattr(model.config, length_setting, None)\n","        if max_length:\n","            print(f\"Found max lenth: {max_length}\")\n","            break\n","    if not max_length:\n","        max_length = 1024\n","        print(f\"Using default max length: {max_length}\")\n","    return max_length\n","\n","\n","def preprocess_batch(batch, tokenizer, max_length):\n","    \"\"\"\n","    Tokenizing a batch\n","    \"\"\"\n","    return tokenizer(\n","        batch[\"text\"],\n","        max_length=max_length,\n","        truncation=True,\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["This function, `preprocess_dataset`, prepares a dataset for training by formatting and tokenizing it, ensuring that it is ready for consumption by a language model.\n","\n","### Parameters\n","- `tokenizer` (AutoTokenizer): The tokenizer associated with the language model.\n","- `max_length` (int): The maximum number of tokens to emit from the tokenizer.\n","- `seed` (int): The random seed used for shuffling the dataset.\n","- `dataset`: The dataset to be preprocessed.\n","\n","### Function Steps\n","1. Add a prompt to each sample in the dataset using the `create_prompt_formats` function.\n","2. Define a partial function `_preprocessing_function` using `preprocess_batch`, `max_length`, and `tokenizer`.\n","3. Apply the `_preprocessing_function` to each batch in the dataset to tokenize and format the samples.\n","4. Remove unnecessary columns ('id', 'topic', 'dialogue', 'summary') from the dataset.\n","5. Filter out samples with input_ids exceeding the specified `max_length`.\n","6. Shuffle the dataset using the provided random seed."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:34:45.771345Z","iopub.status.busy":"2024-02-27T08:34:45.770958Z","iopub.status.idle":"2024-02-27T08:34:45.919970Z","shell.execute_reply":"2024-02-27T08:34:45.918411Z","shell.execute_reply.started":"2024-02-27T08:34:45.771317Z"},"trusted":true},"outputs":[],"source":["from functools import partial\n","\n","# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n","def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n","    \"\"\"Format & tokenize it so it is ready for training\n","    :param tokenizer (AutoTokenizer): Model Tokenizer\n","    :param max_length (int): Maximum number of tokens to emit from tokenizer\n","    \"\"\"\n","    \n","    # Add prompt to each sample\n","    print(\"Preprocessing dataset...\")\n","    dataset = dataset.map(create_prompt_formats)#, batched=True)\n","    \n","    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n","    dataset = dataset.map(\n","        _preprocessing_function,\n","        batched=True,\n","        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n","    )\n","\n","    # Filter out samples that have input_ids exceeding max_length\n","    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n","    \n","    # Shuffle dataset\n","    dataset = dataset.shuffle(seed=seed)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:34:45.922841Z","iopub.status.busy":"2024-02-27T08:34:45.921527Z","iopub.status.idle":"2024-02-27T08:34:46.006833Z","shell.execute_reply":"2024-02-27T08:34:46.005554Z","shell.execute_reply.started":"2024-02-27T08:34:45.922809Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU memory occupied: 2596 MB.\n"]}],"source":["print_gpu_utilization()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:34:46.010856Z","iopub.status.busy":"2024-02-27T08:34:46.010395Z","iopub.status.idle":"2024-02-27T08:34:59.541061Z","shell.execute_reply":"2024-02-27T08:34:59.540097Z","shell.execute_reply.started":"2024-02-27T08:34:46.010819Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found max lenth: 2048\n","2048\n","Preprocessing dataset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89a9f9bd52d44c90af39da4bea811067","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ce49955020b48ec934b5772baa340b1","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"edf6cf38749e45dda0fafa8cedb7cf68","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Preprocessing dataset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"465b16cc80004c49a891b54ea9c51e1b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb3a7ad99d53437385a462549f203a0c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90892a42cb3b45c783326155c3529109","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# ## Pre-process dataset\n","max_length = get_max_length(original_model)\n","print(max_length)\n","\n","train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n","eval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:34:59.542514Z","iopub.status.busy":"2024-02-27T08:34:59.542218Z","iopub.status.idle":"2024-02-27T08:34:59.548251Z","shell.execute_reply":"2024-02-27T08:34:59.547247Z","shell.execute_reply.started":"2024-02-27T08:34:59.542465Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shapes of the datasets:\n","Training: (1999, 3)\n","Validation: (499, 3)\n","Dataset({\n","    features: ['text', 'input_ids', 'attention_mask'],\n","    num_rows: 1999\n","})\n"]}],"source":["print(f\"Shapes of the datasets:\")\n","print(f\"Training: {train_dataset.shape}\")\n","print(f\"Validation: {eval_dataset.shape}\")\n","print(train_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["<a name='8'></a>\n","#### 8. Setup the PEFT/QLoRA model for Fine-Tuning\n","Now, let's perform Parameter Efficient Fine-Tuning (PEFT) fine-tuning. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning.\n","PEFT is a generic term that includes Low-Rank Adaptation (LoRA) and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, in essence, enables efficient model fine-tuning using fewer computational resources, often achievable with just a single GPU. Following LoRA fine-tuning for a specific task or use case, the outcome is an unchanged original LLM and the emergence of a considerably smaller \"LoRA adapter,\" often representing a single-digit percentage of the original LLM size (in MBs rather than GBs).\n","\n","\n","Note the rank (r) hyper-parameter, which defines the rank/dimension of the adapter to be trained.\n","r is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n","\n","alpha is the scaling factor for the learned weights. The weight matrix is scaled by alpha/r, and thus a higher value for alpha assigns more weight to the LoRA activations."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def print_number_of_trainable_model_parameters(model):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n","\n","#print(print_number_of_trainable_model_parameters(original_model))"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:34:59.567222Z","iopub.status.busy":"2024-02-27T08:34:59.566612Z","iopub.status.idle":"2024-02-27T08:34:59.578842Z","shell.execute_reply":"2024-02-27T08:34:59.577899Z","shell.execute_reply.started":"2024-02-27T08:34:59.567196Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PhiForCausalLM(\n","  (model): PhiModel(\n","    (embed_tokens): Embedding(51200, 2560)\n","    (embed_dropout): Dropout(p=0.0, inplace=False)\n","    (layers): ModuleList(\n","      (0-31): 32 x PhiDecoderLayer(\n","        (self_attn): PhiAttention(\n","          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (rotary_emb): PhiRotaryEmbedding()\n","        )\n","        (mlp): PhiMLP(\n","          (activation_fn): NewGELUActivation()\n","          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n","          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n","        )\n","        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","        (resid_dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",")\n"]}],"source":["print(original_model)"]},{"cell_type":"markdown","metadata":{},"source":["The PhiForCausalLM model is a variant of a causal language model designed for text generation tasks. It utilizes the PhiModel architecture, which includes embedding layers, multiple layers of PhiDecoderLayer, and a final linear layer (lm_head).\n","\n","Key components of the model include:\n","\n","1. Embedding layer: Maps input tokens to continuous vector representations.\n","2. PhiDecoderLayer: Consists of self-attention mechanisms and multi-layer perceptrons (MLPs) for capturing contextual information and modeling dependencies between tokens.\n","3. Final linear layer (lm_head): Predicts the probability distribution over the vocabulary for generating the next token in the sequence.\n","\n","Overall, the PhiForCausalLM model is tailored for generating coherent and contextually relevant text sequences, making it suitable for various natural language processing tasks such as text summarization, dialogue generation, and machine translation."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:34:59.580056Z","iopub.status.busy":"2024-02-27T08:34:59.579810Z","iopub.status.idle":"2024-02-27T08:35:00.107330Z","shell.execute_reply":"2024-02-27T08:35:00.106419Z","shell.execute_reply.started":"2024-02-27T08:34:59.580035Z"},"trusted":true},"outputs":[],"source":["from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","config = LoraConfig(\n","    r=32, #Rank\n","    lora_alpha=32,\n","    target_modules=[\n","        'q_proj',\n","        'k_proj',\n","        'v_proj',\n","        'dense'\n","    ],\n","    bias=\"none\",\n","    lora_dropout=0.05,  # Conventional\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n","original_model.gradient_checkpointing_enable()\n","\n","# 2 - Using the prepare_model_for_kbit_training method from PEFT\n","original_model = prepare_model_for_kbit_training(original_model)\n","\n","peft_model = get_peft_model(original_model, config)"]},{"cell_type":"markdown","metadata":{},"source":["Once everything is set up and the base model is prepared, we can use the print_trainable_parameters() helper function to see how many trainable parameters are in the model."]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:35:00.108860Z","iopub.status.busy":"2024-02-27T08:35:00.108566Z","iopub.status.idle":"2024-02-27T08:35:00.120888Z","shell.execute_reply":"2024-02-27T08:35:00.119798Z","shell.execute_reply.started":"2024-02-27T08:35:00.108826Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable model parameters: 20971520\n","all model parameters: 1542364160\n","percentage of trainable model parameters: 1.36%\n"]}],"source":["print(print_number_of_trainable_model_parameters(peft_model))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:35:00.125917Z","iopub.status.busy":"2024-02-27T08:35:00.125370Z","iopub.status.idle":"2024-02-27T08:35:00.141174Z","shell.execute_reply":"2024-02-27T08:35:00.140272Z","shell.execute_reply.started":"2024-02-27T08:35:00.125889Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): PhiForCausalLM(\n","      (model): PhiModel(\n","        (embed_tokens): Embedding(51200, 2560)\n","        (embed_dropout): Dropout(p=0.0, inplace=False)\n","        (layers): ModuleList(\n","          (0-31): 32 x PhiDecoderLayer(\n","            (self_attn): PhiAttention(\n","              (q_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (k_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (v_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (dense): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (rotary_emb): PhiRotaryEmbedding()\n","            )\n","            (mlp): PhiMLP(\n","              (activation_fn): NewGELUActivation()\n","              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n","              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n","            )\n","            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n","    )\n","  )\n",")\n"]}],"source":["# See how the model looks different now, with the QLoRA adapters added:\n","print(peft_model)"]},{"cell_type":"markdown","metadata":{},"source":["<a name='9'></a>\n","#### 9. Train PEFT Adapter\n","\n","Define training arguments and create Trainer instance."]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:35:00.142856Z","iopub.status.busy":"2024-02-27T08:35:00.142560Z","iopub.status.idle":"2024-02-27T08:35:00.157396Z","shell.execute_reply":"2024-02-27T08:35:00.156600Z","shell.execute_reply.started":"2024-02-27T08:35:00.142825Z"},"trusted":true},"outputs":[],"source":["output_dir = './peft-dialogue-summary-training/final-checkpoint'\n","import transformers\n","\n","peft_training_args = TrainingArguments(\n","    output_dir = output_dir,\n","    warmup_steps=1,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    max_steps=1000,\n","    learning_rate=2e-4,\n","    optim=\"paged_adamw_8bit\",\n","    logging_steps=25,\n","    logging_dir=\"./logs\",\n","    save_strategy=\"steps\",\n","    save_steps=25,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=25,\n","    do_eval=True,\n","    gradient_checkpointing=True,\n","    report_to=\"none\",\n","    overwrite_output_dir = 'True',\n","    group_by_length=True,\n",")\n","\n","peft_model.config.use_cache = False\n","\n","peft_trainer = transformers.Trainer(\n","    model=peft_model,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    args=peft_training_args,\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T08:35:00.158669Z","iopub.status.busy":"2024-02-27T08:35:00.158342Z","iopub.status.idle":"2024-02-27T08:35:00.167193Z","shell.execute_reply":"2024-02-27T08:35:00.166249Z","shell.execute_reply.started":"2024-02-27T08:35:00.158645Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda', index=0)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["peft_training_args.device"]},{"cell_type":"code","execution_count":25,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-02-27T08:35:00.168660Z","iopub.status.busy":"2024-02-27T08:35:00.168275Z","iopub.status.idle":"2024-02-27T12:52:35.310655Z","shell.execute_reply":"2024-02-27T12:52:35.309698Z","shell.execute_reply.started":"2024-02-27T08:35:00.168625Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1000/1000 4:17:14, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>1.623600</td>\n","      <td>1.378991</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.203200</td>\n","      <td>1.408253</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>1.430400</td>\n","      <td>1.345791</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.150300</td>\n","      <td>1.364201</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>1.467700</td>\n","      <td>1.336263</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.189000</td>\n","      <td>1.342465</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>1.421100</td>\n","      <td>1.331966</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.192900</td>\n","      <td>1.337227</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>1.449300</td>\n","      <td>1.329550</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.191100</td>\n","      <td>1.339475</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>1.462000</td>\n","      <td>1.326767</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.148400</td>\n","      <td>1.333722</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>1.349800</td>\n","      <td>1.325653</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.130200</td>\n","      <td>1.332706</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>1.429800</td>\n","      <td>1.323737</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.159800</td>\n","      <td>1.328201</td>\n","    </tr>\n","    <tr>\n","      <td>425</td>\n","      <td>1.384800</td>\n","      <td>1.322916</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>1.151000</td>\n","      <td>1.324833</td>\n","    </tr>\n","    <tr>\n","      <td>475</td>\n","      <td>1.402700</td>\n","      <td>1.320308</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.141000</td>\n","      <td>1.324416</td>\n","    </tr>\n","    <tr>\n","      <td>525</td>\n","      <td>1.403500</td>\n","      <td>1.320901</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>1.132700</td>\n","      <td>1.328239</td>\n","    </tr>\n","    <tr>\n","      <td>575</td>\n","      <td>1.356300</td>\n","      <td>1.321690</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.123300</td>\n","      <td>1.325854</td>\n","    </tr>\n","    <tr>\n","      <td>625</td>\n","      <td>1.369000</td>\n","      <td>1.321348</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>1.128100</td>\n","      <td>1.325458</td>\n","    </tr>\n","    <tr>\n","      <td>675</td>\n","      <td>1.375200</td>\n","      <td>1.319792</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>1.093100</td>\n","      <td>1.322809</td>\n","    </tr>\n","    <tr>\n","      <td>725</td>\n","      <td>1.397600</td>\n","      <td>1.318833</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>1.082800</td>\n","      <td>1.322228</td>\n","    </tr>\n","    <tr>\n","      <td>775</td>\n","      <td>1.362800</td>\n","      <td>1.319142</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>1.090100</td>\n","      <td>1.323303</td>\n","    </tr>\n","    <tr>\n","      <td>825</td>\n","      <td>1.365700</td>\n","      <td>1.320103</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>1.098100</td>\n","      <td>1.322426</td>\n","    </tr>\n","    <tr>\n","      <td>875</td>\n","      <td>1.340600</td>\n","      <td>1.320126</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.085500</td>\n","      <td>1.321977</td>\n","    </tr>\n","    <tr>\n","      <td>925</td>\n","      <td>1.378700</td>\n","      <td>1.320689</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>1.068200</td>\n","      <td>1.321478</td>\n","    </tr>\n","    <tr>\n","      <td>975</td>\n","      <td>1.346600</td>\n","      <td>1.320799</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.088200</td>\n","      <td>1.320657</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["TrainOutput(global_step=1000, training_loss=1.2691061172485352, metrics={'train_runtime': 15453.7563, 'train_samples_per_second': 0.518, 'train_steps_per_second': 0.065, 'total_flos': 3.712609867585536e+16, 'train_loss': 1.2691061172485352, 'epoch': 4.0})"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["peft_trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["* The training loss decreases gradually over time, indicating that the model is learning from the training data.\n","* The validation loss fluctuates but generally follows a similar trend to the training loss, suggesting that the model's performance on unseen data is consistent with its performance on the training data.\n","* There is no significant divergence between the training and validation losses, indicating that the model is not overfitting or underfitting severely.\n","* Overall, the training results suggest that the model is learning effectively from the data without exhibiting clear signs of underfitting or overfitting. However, further analysis may be needed to optimize the model's performance."]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T12:52:35.312320Z","iopub.status.busy":"2024-02-27T12:52:35.311718Z","iopub.status.idle":"2024-02-27T12:52:35.317266Z","shell.execute_reply":"2024-02-27T12:52:35.316388Z","shell.execute_reply.started":"2024-02-27T12:52:35.312281Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU memory occupied: 14760 MB.\n"]}],"source":["print_gpu_utilization()"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T12:52:35.318812Z","iopub.status.busy":"2024-02-27T12:52:35.318452Z","iopub.status.idle":"2024-02-27T12:52:35.525275Z","shell.execute_reply":"2024-02-27T12:52:35.524601Z","shell.execute_reply.started":"2024-02-27T12:52:35.318781Z"},"trusted":true},"outputs":[],"source":["# Free memory for merging weights\n","del original_model\n","del peft_trainer\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T12:52:35.526705Z","iopub.status.busy":"2024-02-27T12:52:35.526415Z","iopub.status.idle":"2024-02-27T12:52:35.531499Z","shell.execute_reply":"2024-02-27T12:52:35.530515Z","shell.execute_reply.started":"2024-02-27T12:52:35.526682Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU memory occupied: 3584 MB.\n"]}],"source":["print_gpu_utilization()"]},{"cell_type":"markdown","metadata":{},"source":["<a name='10'></a>\n","#### 10. Evaluate the Model Qualitatively (Human Evaluation)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T12:52:35.532905Z","iopub.status.busy":"2024-02-27T12:52:35.532644Z","iopub.status.idle":"2024-02-27T12:52:39.150507Z","shell.execute_reply":"2024-02-27T12:52:39.149710Z","shell.execute_reply.started":"2024-02-27T12:52:35.532882Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0edde3fec8f4cc7b4b27a37fde96457","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","base_model_id = \"microsoft/phi-2\"\n","base_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n","                                                      device_map='auto',\n","                                                      quantization_config=bnb_config,\n","                                                      trust_remote_code=True,\n","                                                      use_auth_token=True)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T12:52:39.151894Z","iopub.status.busy":"2024-02-27T12:52:39.151610Z","iopub.status.idle":"2024-02-27T12:52:39.294019Z","shell.execute_reply":"2024-02-27T12:52:39.293235Z","shell.execute_reply.started":"2024-02-27T12:52:39.151868Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n","eval_tokenizer.pad_token = eval_tokenizer.eos_token"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T12:52:39.295430Z","iopub.status.busy":"2024-02-27T12:52:39.295129Z","iopub.status.idle":"2024-02-27T12:52:39.831604Z","shell.execute_reply":"2024-02-27T12:52:39.830774Z","shell.execute_reply.started":"2024-02-27T12:52:39.295403Z"},"trusted":true},"outputs":[],"source":["from peft import PeftModel\n","\n","ft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T12:52:39.833035Z","iopub.status.busy":"2024-02-27T12:52:39.832773Z","iopub.status.idle":"2024-02-27T12:52:49.533215Z","shell.execute_reply":"2024-02-27T12:52:49.532238Z","shell.execute_reply.started":"2024-02-27T12:52:39.833012Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","Instruct: Summarize the following conversation.\n","#Person1#: Happy Birthday, this is for you, Brian.\n","#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n","#Person1#: Brian, may I have a pleasure to have a dance with you?\n","#Person2#: Ok.\n","#Person1#: This is really wonderful party.\n","#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n","#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n","#Person2#: You look great, you are absolutely glowing.\n","#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n","Output:\n","\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n","\n","---------------------------------------------------------------------------------------------------\n","PEFT MODEL:\n","#Person1# brings a birthday gift for Brian and they have a dance together. #Person1# compliments Brian and they decide to have a drink together.\n","\n","### End of Output.\n","\n","### \n","CPU times: user 9.67 s, sys: 23.2 ms, total: 9.69 s\n","Wall time: 9.69 s\n"]}],"source":["%%time\n","from transformers import set_seed\n","set_seed(seed)\n","\n","index = 10\n","dialogue = dataset['test'][index]['dialogue']\n","summary = dataset['test'][index]['summary']\n","\n","prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n","\n","peft_model_res = gen(ft_model,prompt,100,)\n","peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n","#print(peft_model_output)\n","prefix, success, result = peft_model_output.partition('#End')\n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{prompt}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n","print(dash_line)\n","print(f'PEFT MODEL:\\n{prefix}')"]},{"cell_type":"markdown","metadata":{},"source":["<a name='11'></a>\n","#### 11. Evaluate the Model Quantitatively (with ROUGE Metric)\n","Perform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). "]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T12:52:49.534903Z","iopub.status.busy":"2024-02-27T12:52:49.534535Z","iopub.status.idle":"2024-02-27T12:52:53.143390Z","shell.execute_reply":"2024-02-27T12:52:53.142630Z","shell.execute_reply.started":"2024-02-27T12:52:49.534869Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"693074ec02254d2f8c10e7f1615c15eb","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n","                                                      device_map='auto',\n","                                                      quantization_config=bnb_config,\n","                                                      trust_remote_code=True,\n","                                                      use_auth_token=True)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T12:52:53.144972Z","iopub.status.busy":"2024-02-27T12:52:53.144653Z","iopub.status.idle":"2024-02-27T12:55:19.061134Z","shell.execute_reply":"2024-02-27T12:55:19.060112Z","shell.execute_reply.started":"2024-02-27T12:52:53.144943Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>human_baseline_summaries</th>\n","      <th>original_model_summaries</th>\n","      <th>peft_model_summaries</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n","      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>In order to prevent employees from wasting tim...</td>\n","      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n","      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>#Person2# arrives late because of traffic jam....</td>\n","      <td>Person1 and Person2 are discussing the traffic...</td>\n","      <td>#Person2# got stuck in traffic again and #Pers...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n","      <td>Person1 and Person2 are discussing the traffic...</td>\n","      <td>#Person2# got stuck in traffic again and #Pers...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>#Person2# complains to #Person1# about the tra...</td>\n","      <td>Person1 and Person2 are discussing the traffic...</td>\n","      <td>#Person2# got stuck in traffic again and #Pers...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n","      <td>Kate informed that Masha and Hero are getting ...</td>\n","      <td>Masha and Hero are getting divorced. Masha tel...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n","      <td>Kate informed that Masha and Hero are getting ...</td>\n","      <td>Masha and Hero are getting divorced. Masha tel...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>#Person1# and Kate talk about the divorce betw...</td>\n","      <td>Kate informed that Masha and Hero are getting ...</td>\n","      <td>Masha and Hero are getting divorced. Masha tel...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>#Person1# and Brian are at the birthday party ...</td>\n","      <td>Person1 and Person2 are at a party, and Person...</td>\n","      <td>#Person1# brings a birthday gift for Brian and...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            human_baseline_summaries  \\\n","0  Ms. Dawson helps #Person1# to write a memo to ...   \n","1  In order to prevent employees from wasting tim...   \n","2  Ms. Dawson takes a dictation for #Person1# abo...   \n","3  #Person2# arrives late because of traffic jam....   \n","4  #Person2# decides to follow #Person1#'s sugges...   \n","5  #Person2# complains to #Person1# about the tra...   \n","6  #Person1# tells Kate that Masha and Hero get d...   \n","7  #Person1# tells Kate that Masha and Hero are g...   \n","8  #Person1# and Kate talk about the divorce betw...   \n","9  #Person1# and Brian are at the birthday party ...   \n","\n","                            original_model_summaries  \\\n","0  Person 1: Ms. Dawson, I need you to take a dic...   \n","1  Person 1: Ms. Dawson, I need you to take a dic...   \n","2  Person 1: Ms. Dawson, I need you to take a dic...   \n","3  Person1 and Person2 are discussing the traffic...   \n","4  Person1 and Person2 are discussing the traffic...   \n","5  Person1 and Person2 are discussing the traffic...   \n","6  Kate informed that Masha and Hero are getting ...   \n","7  Kate informed that Masha and Hero are getting ...   \n","8  Kate informed that Masha and Hero are getting ...   \n","9  Person1 and Person2 are at a party, and Person...   \n","\n","                                peft_model_summaries  \n","0  #Person1# asks Ms. Dawson to take a dictation ...  \n","1  #Person1# asks Ms. Dawson to take a dictation ...  \n","2  #Person1# asks Ms. Dawson to take a dictation ...  \n","3  #Person2# got stuck in traffic again and #Pers...  \n","4  #Person2# got stuck in traffic again and #Pers...  \n","5  #Person2# got stuck in traffic again and #Pers...  \n","6  Masha and Hero are getting divorced. Masha tel...  \n","7  Masha and Hero are getting divorced. Masha tel...  \n","8  Masha and Hero are getting divorced. Masha tel...  \n","9  #Person1# brings a birthday gift for Brian and...  "]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","dialogues = dataset['test'][0:10]['dialogue']\n","human_baseline_summaries = dataset['test'][0:10]['summary']\n","\n","original_model_summaries = []\n","instruct_model_summaries = []\n","peft_model_summaries = []\n","\n","for idx, dialogue in enumerate(dialogues):\n","    human_baseline_text_output = human_baseline_summaries[idx]\n","    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n","    \n","    original_model_res = gen(original_model,prompt,100,)\n","    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n","    \n","    peft_model_res = gen(ft_model,prompt,100,)\n","    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n","    #print(peft_model_output)\n","    peft_model_text_output, success, result = peft_model_output.partition('#End')\n","    \n","\n","    original_model_summaries.append(original_model_text_output)\n","    peft_model_summaries.append(peft_model_text_output)\n","\n","zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n"," \n","df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n","df"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T12:55:19.062763Z","iopub.status.busy":"2024-02-27T12:55:19.062403Z","iopub.status.idle":"2024-02-27T12:55:33.942495Z","shell.execute_reply":"2024-02-27T12:55:33.941449Z","shell.execute_reply.started":"2024-02-27T12:55:19.062734Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.24.4)\n","Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=12eaf119dbe7c54ec6806e08989fa74cbc6e30fae70718af72f3fed6e1d5beb0\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}],"source":["!pip install rouge_score"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T12:55:33.945206Z","iopub.status.busy":"2024-02-27T12:55:33.944323Z","iopub.status.idle":"2024-02-27T12:55:37.373974Z","shell.execute_reply":"2024-02-27T12:55:37.372859Z","shell.execute_reply.started":"2024-02-27T12:55:33.945164Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b61308c6382945989eb7f6b2113af4b4","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["ORIGINAL MODEL:\n","{'rouge1': 0.2827007613710234, 'rouge2': 0.09658938946924345, 'rougeL': 0.20147734920398974, 'rougeLsum': 0.21266882373948465}\n","PEFT MODEL:\n","{'rouge1': 0.319583317792019, 'rouge2': 0.10724176116995074, 'rougeL': 0.23977189826979742, 'rougeLsum': 0.25606007569886413}\n"]}],"source":["import evaluate\n","\n","rouge = evaluate.load('rouge')\n","\n","original_model_results = rouge.compute(\n","    predictions=original_model_summaries,\n","    references=human_baseline_summaries[0:len(original_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","peft_model_results = rouge.compute(\n","    predictions=peft_model_summaries,\n","    references=human_baseline_summaries[0:len(peft_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","print('ORIGINAL MODEL:')\n","print(original_model_results)\n","print('PEFT MODEL:')\n","print(peft_model_results)"]},{"cell_type":"markdown","metadata":{},"source":["The PEFT model shows improvements over the original model across all ROUGE metrics:\n","\n","* ROUGE-1: 0.2827 -> 0.3196 [performs better in capturing unigram overlap between the generated summaries and the reference summaries]\n","* ROUGE-2: 0.0966 -> 0.1072 [indicating improved performance in capturing bigram overlap]\n","* ROUGE-L: 0.2015 -> 0.2398 \n","* ROUGE-Lsum: 0.2127 -> 0.2561 [demonstrates superior performance in capturing long-range dependencies and overall content overlap compared to the original model.]\n","\n","These improvements indicate that the PEFT model generates summaries with better recall and precision compared to the original model."]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T12:55:37.375656Z","iopub.status.busy":"2024-02-27T12:55:37.375265Z","iopub.status.idle":"2024-02-27T12:55:37.386699Z","shell.execute_reply":"2024-02-27T12:55:37.382545Z","shell.execute_reply.started":"2024-02-27T12:55:37.375620Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n","rouge1: 3.69%\n","rouge2: 1.07%\n","rougeL: 3.83%\n","rougeLsum: 4.34%\n"]}],"source":["print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n","\n","improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n","for key, value in zip(peft_model_results.keys(), improvement):\n","    print(f'{key}: {value*100:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
